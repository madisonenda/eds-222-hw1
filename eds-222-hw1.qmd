---
title: "EDS 222: Homework 1"
date: "Assigned #1, due 10/14/24"
author: Madison Enda
editor_options: 
  chunk_output_type: inline
---

## Background

*(The case study in this exercise is based on reality, but does not include actual observational data.)*

In this exercise we will look at a case study concerning air quality in South Asia. The World Health Organization estimates that air pollution kills an estimated seven million people per year, due to its effects on the cardiovascular and respiratory systems. Out of the 40 most polluted cities in the world, South Asia is home to 37, and Pakistan was ranked to contain the second most air pollution in the world in 2020 (IQAIR, 2020). In 2019, Lahore, Pakistan was the 12th most polluted city in the world, exposing a population of 11.1 million people to increased mortality and morbidity risks.

In this exercise, you are given two datasets from Lahore, Pakistan and are asked to compare the two different data collection strategies from this city. These data are:

-   Crowd-sourced data from air quality monitors located in people's homes. These data are voluntarily collected by individual households who choose to install a monitor in their home and upload their data for public access.

-   Official government data from monitors installed by government officials at selected locations across Lahore. There have been reports that government officials strategically locate monitors in locations with cleaner air in order to mitigate domestic and international pressure to clean up the air.

::: callout-note
All data for EDS 222 will be stored on the Taylor server, in the shared `/courses/eds-222/data/` directory. Please see material from EDS 214 on how to access and retrieve data from Taylor. These data are small; all compute can be handled locally. Thanks to Bren PhD student Fatiq Nadeem for assembling these data!
:::

In answering the following questions, please consider the lecture content from class on sampling strategies, as well as the material in Chapter 2 of [*Introduction to Modern Statistics*](https://openintro-ims.netlify.app/data-design). Include in your submission your version of this file "`eds-222-hw1.qmd`" and the rendered HTML output, each containing complete answers to all questions *as well as the associated code*. Questions with answers unsupported by the code will be marked incomplete. Showing your work this way will help you develop the habit of creating reproducible code.

## Assessment 1

### Question 1

Load the data from each source and label it as `crowdsourced` and `govt` accordingly. For example:

```{r}
crowdsourced <- readRDS(file.path("data", "airpol-PK-crowdsourced.RDS"))
govt <- readRDS(file.path("data", "airpol-PK-govt.RDS"))
```

```{r}
# Load libraries 
library(tidyverse)
library(dplyr)
library(splitstackshape)
```

::: callout-warning
There's an implicit assumption about file organization in the code above. What is it? How can you make the code work?
:::

-   The code assumes that you have the data downloaded into the 'data' file in your current directory

1.  These data frames have one row per pollution observation. How many pollution records are in each data set?

```{r}
# Checking the number of rows in our crowd sourced data frame
print(nrow(crowdsourced))

# Checking the number of rows in our government data frame
print(nrow(govt))
```

-   There appear to be 5488 pollution observations for the crowd sourced data, and 1960 for the government data.

2.  Each monitor is located at a unique latitude and longitude location. How many unique monitors are in each dataset?

```{r}
# Finding the number of unique values in the longitude and latitude columns of crowdsourced data
monitor_locations_c <- crowdsourced %>%
  group_by(longitude, latitude) %>%
  mutate(group_id= cur_group_id())

# Counting the number of ids for the number of monitors
crowd_unique_ids <-(monitor_locations_c$group_id)

length(unique(crowd_unique_ids))
```

-   In the crowd sourced data set, there appear to be 14 unique combinations of latitude and longitude, so there are likely 14 monitors.

```{r}
# Finding the number of unique values in the longitude and latitude columns of the government data
monitor_locations_g <- govt %>%
  group_by(longitude, latitude) %>%
  mutate(group_id= cur_group_id())

# Counting the number of ids for the number of monitors
govt_unique_ids <-(monitor_locations_g$group_id)

length(unique(govt_unique_ids))
```

-   In the government data set, there are only 5 unique combinations of latitudes and longitudes, so there are likely 5 monitors.

::: callout-tip
`group_by(longitude,latitude)` and `cur_group_id()` in `dplyr` will help in creating a unique identifier for each (longitude, latitude) pair.
:::

### Question 2

The goal of pollution monitoring in Lahore is to measure the average pollution conditions across the city.

1.  What is the *population* in this setting? Please be precise.

    -   The population in this setting is the entire volume of air in the city of Lahore (the atmosphere encompassing the city) from November of 2018 to November of 2019 (1 year).

2.  What are the *samples* in this setting? Please be precise.

    -   The samples are the crowd-sourced data, and the government sourced data.

3.  These samples were not randomly collected from across locations in Lahore. Given the sampling approaches described above, discuss possible biases that may enter when we use these samples to construct estimates of population parameters.

    -   Samples that are not randomly collected will introduce bias into our analysis, as we may associate changes in our pollution levels to the wrong variables ( all change in y will be associated to x as we omit other potential variables). Therefore, our summary statistics will be less representative of the actual values of the population.

### Question 3

1.  For both the government data and the crowd-sourced data, report the sample mean, sample minimum, and sample maximum value of PM 2.5 (measured in $\mu g/m^3$).

    ```{r}
    # Finding summary statistics for the PM 2.5 government data
    print(mean(govt[["PM"]]))

    print(max(govt[["PM"]]))

    print(min(govt[["PM"]]))
    ```

    -   For government data set: mean= 39.6, max= 65, min= 15

```{r}
# Finding summary statistics for the PM 2.5 crowdsourced data
print(mean(crowdsourced[["PM"]]))

print(max(crowdsourced[["PM"]]))

print(min(crowdsourced[["PM"]])) 

```

-   For crowd sourced data set: mean= 70.2, max= 120, min= 20

-   Discuss any key differences that you see between these two samples.

    -   The crowd sourced data set had a much higher mean and a larger range for the PM column, as well as many more observations than the government data. This could indicate the sample size of the government data introduces bias (operating under the assumption that as sample size approaches infinity, sample mean approaches the actual mean of the population), by misrepresenting the sample statistics.

-   Are the differences in mean pollution as expected, given what we know about the sampling strategies?

    -   Yes, it would make sense that there are differences occurring in the sample means from the two data sets. The number of monitors and observances for the crowd sourced data is higher, and therefore their sample mean may be more accurate, as increased sample size increases the accuracy of our mean.

### Question 4

Use the location of the air pollution stations for both of the sampling strategies to generate a map showing locations of each observation. Color the two samples with different colors to highlight how each sample obtains measurements from different parts of the city.

::: callout-tip
`longitude` indicates location in the *x*-direction, while `latitude` indicates location in the *y*-direction. With `ggplot2` this should be nothing fancy. We'll do more spatial data in `R` later in the course.
:::

```{r}
# Making the crowd sourced monitor map 
ggplot() +
  geom_point(data= crowdsourced, aes(x= longitude, y= latitude, color= "red")) +
  geom_point(data= govt, aes(x= longitude, y= latitude, color= "blue"))+ 
  labs(title= "Location of Crowdsourced Air Quality Monitors in Lahore",
      x= "Longitude",
      y= "Latitude")+
  scale_color_hue(labels= c("Government Sites", "Crowdsourced Sites"))
  
```

### Question 5

The local newspaper in Pakistan, *Dawn*, claims that the government is misreporting the air pollution levels in Lahore. Do the locations of monitors in question 4, relative to crowd-sourced monitors, suggest anything about a possible political bias?

-   Since there are less monitors relative to the crowd sourced data, the government of Pakistan does not appear to be investing a lot of time or money into air quality monitoring in Lahore. This limited sample size leads to misleading statistics, whether unintentional or purposeful/malicious.

### Question 6

Given the recent corruption in air quality reporting, the Prime Minister of Pakistan has hired an independent body of environmental data scientists to create an unbiased estimate of the mean PM 2.5 across Lahore using some combination of both government stations and crowd sourced observations.

NASA's satellite data indicates that the average PM across Lahore is 89.2 $\mu g/m^3$. Since this is the most objective estimate of population-level PM 2.5 available, your goal is to match this mean as closely as possible by creating a new ground-level monitoring sample that draws on both the government and crowd-sourced samples.

#### Question 6.1

First, generate a *random sample* of size $n=1000$ air pollution records by (i) pooling observations across the government and the crowd-sourced data; and (ii) drawing observations at random from this pooled sample.

::: callout-tip
`bind_rows()` may be helpful.
:::

```{r}
# Binding our two data frames together using bind_rows
all_air_data <- bind_rows(crowdsourced, govt)


# Getting a random sample of 1000 values from the combined data
sample_PM <- sample_n(all_air_data, 1000)

# Printing the dimensions of the data set for random sampling
dim(sample_PM)
```

Second, create a *stratified random sample*. Do so by (i) stratifying your pooled data-set into strata of 0.01 degrees of latitude, and (ii) randomly sampling 200 air pollution observations from each stratum.

```{r}
# Randomly sampling 200 air pollution observations from each strata
set.seed(4321)

all_air<-all_air_data %>%
  mutate(lat_r = round(latitude, digits= 2))

stratified_sample <- all_air %>%
  group_by(lat_r) %>%
  slice_sample(n= 40)

# Printing the dimensions of the data set for stratified sampling
dim(stratified_sample)
```


#### Question 6.2

Compare estimated means of PM 2.5 for each sampling strategy to the NASA estimate of 89.2 $\mu g/m^3$. Which sample seems to match the satellite data best? What would you recommend the Prime Minister do? Does your proposed sampling strategy rely more on government or on crowd-sourced data? Why might that be the case?

```{r}
#Finding the mean of 1000 random samples
print(mean(sample_PM[["PM"]])) 

# Finding the mean of the 200 stratified random samples

print(mean(stratified_sample[["PM"]]))
  
```

-   The random sampling from the pooled data set (both the government and crowd-sourced samples) produced a mean closer to the NASA value of 89.2 than the original government data set alone. Using the stratified sampling method for the pooled data set, we were able to obtain an even closer value than the random sampling, and thus would be the best fit. I would suggest the Prime Minister employ a stratified sampling method for analyzing air pollution in Pakistan, if possible matching the sample size of the crowd-sourced data, as the accuracy of the mean increases with increased sample size.
